# Diffusion Policy ëª¨ë¸ êµ¬ì¡° ë° ì…ì¶œë ¥ íë¦„

> ê¸°ì¤€ ì½”ë“œ: `src/lerobot/policies/diffusion/`
> RoboCasa PnPC2M ë°ì´í„°ì…‹ ê¸°ì¤€ìœ¼ë¡œ ì‹¤ì œ í…ì„œ shapeì„ ëª…ì‹œ

---

## ì „ì²´ íë¦„ ìš”ì•½

```
[ë°ì´í„°ì…‹]
  observation.images.*   (B, n_obs=2, 3_cams, 3, 128, 128)
  observation.state      (B, n_obs=2, state_dim)
  action                 (B, horizon=16, 11)
        â†“
[ì „ì²˜ë¦¬]
  ì´ë¯¸ì§€: crop â†’ ì •ê·œí™” (MEAN_STD)
  state : MIN_MAX ì •ê·œí™”
  action: MIN_MAX ì •ê·œí™”
        â†“
[DiffusionPolicy.forward()]
        â†“
[DiffusionRgbEncoder]   ì´ë¯¸ì§€ â†’ 1D ë²¡í„°
        â†“
[_prepare_global_conditioning]   state + image feature í•©ì¹˜ê¸° â†’ global_cond
        â†“
[DiffusionConditionalUnet1d]   noisy action + global_cond â†’ denoised action
        â†“
  loss = MSE(pred, target)   (prediction_type="epsilon" â†’ ë…¸ì´ì¦ˆ ì˜ˆì¸¡)
```

---

## 1. ë°ì´í„° ì…ë ¥ êµ¬ì¡°

### ë°°ì¹˜ ë”•ì…”ë„ˆë¦¬ (í•™ìŠµ ì‹œ)
| í‚¤ | shape | ì„¤ëª… |
|----|-------|------|
| `observation.images` | `(B, 2, 3, 3, 128, 128)` | ìŠ¤íƒëœ ì¹´ë©”ë¼ (n_obs=2, n_cams=3) |
| `observation.state` | `(B, 2, state_dim)` | n_obs=2 íƒ€ì„ìŠ¤í…ì˜ ë¡œë´‡ state |
| `action` | `(B, 16, 11)` | horizon=16 ê°œì˜ ì•¡ì…˜ ì‹œí€€ìŠ¤ |
| `action_is_pad` | `(B, 16)` | ì—í”¼ì†Œë“œ ëì—ì„œ íŒ¨ë”©ëœ ì•¡ì…˜ ë§ˆìŠ¤í¬ |

> `observation.images`ëŠ” `forward()`ì—ì„œ ê° ì¹´ë©”ë¼ í‚¤ë¥¼ `torch.stack`ìœ¼ë¡œ í•©ì³ì„œ ë§Œë“¦
> (`modeling_diffusion.py:144-145`)

### ì‹œê°„ ì¸ë±ìŠ¤ ê·œì¹™ (delta_indices)
- `observation_delta_indices`: `[-1, 0]` â†’ í˜„ì¬ + 1ìŠ¤í… ì „ ê´€ì¸¡
- `action_delta_indices`: `[-1, 0, 1, ..., 14]` â†’ horizon=16ê°œ ì•¡ì…˜

---

## 2. ì´ë¯¸ì§€ ì¸ì½”ë”: `DiffusionRgbEncoder`

```
ì…ë ¥: (B, 3, H, W)   â† B = batch * n_obs_steps * n_cameras

  1) [crop]
     train: RandomCrop(crop_shape)   â†’ (B, 3, 84, 84)  â† ê¸°ë³¸ê°’
     eval : CenterCrop(crop_shape)   â†’ (B, 3, 84, 84)

  2) [backbone]
     ResNet18 [:-2]                  â†’ (B, 512, 3, 3)   â† 84x84 ê¸°ì¤€
     (ë§ˆì§€ë§‰ avgpool, fc ì œê±°)
     BatchNorm â†’ GroupNorm êµì²´ (use_group_norm=True ì‹œ)

  3) [SpatialSoftmax]
     (B, 512, 3, 3) â†’ 32 keypoints   â†’ (B, 32, 2)
     â†’ flatten                        â†’ (B, 64)

  4) [Linear + ReLU]
                                      â†’ (B, 64)   â† feature_dim

ì¶œë ¥: (B, 64)
```

**í•µì‹¬ íŒŒë¼ë¯¸í„°:**
| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì˜í–¥ |
|---------|--------|------|
| `vision_backbone` | `"resnet18"` | backbone í¬ê¸° (resnet34/50ë„ ê°€ëŠ¥) |
| `crop_shape` | `(84, 84)` | 128Ã—128ì—ì„œ 43% ì˜ë¦¼ â†’ âš ï¸ ê²€í†  í•„ìš” |
| `pretrained_backbone_weights` | `None` | scratch í•™ìŠµ (ImageNet ê°€ì¤‘ì¹˜ ì‚¬ìš© ê°€ëŠ¥) |
| `spatial_softmax_num_keypoints` | `32` | ì¶œë ¥ feature_dim = 32Ã—2 = **64** |
| `use_separate_rgb_encoder_per_camera` | `False` | 3 ì¹´ë©”ë¼ê°€ ë™ì¼ encoder ê³µìœ  |

---

## 3. ê¸€ë¡œë²Œ ì»¨ë””ì…”ë‹: `_prepare_global_conditioning`

3ê°œ ì¹´ë©”ë¼ì™€ stateë¥¼ í•©ì³ UNetì— ë„£ì„ ë‹¨ì¼ ì»¨ë””ì…”ë‹ ë²¡í„°ë¥¼ ë§Œë“¦.

```
[ì´ë¯¸ì§€ ì²˜ë¦¬]
  (B, 2, 3, 3, 128, 128)
  â†’ rearrange: (B*2*3, 3, 128, 128)     â† batch + n_obs + n_cams í•©ì¹˜ê¸°
  â†’ DiffusionRgbEncoder                 â†’ (B*2*3, 64)
  â†’ rearrange: (B, 2, 3*64=192)         â† ì¹´ë©”ë¼ feature ì´ì–´ ë¶™ì´ê¸°

[state]
  (B, 2, state_dim)

[cat + flatten]
  cat([state, img_feats], dim=-1)       â†’ (B, 2, state_dim + 192)
  flatten(start_dim=1)                  â†’ (B, 2 * (state_dim + 192))

ì˜ˆ: state_dim=16  â†’ global_cond = (B, 2*(16+192)) = (B, 416)
ì˜ˆ: state_dim=53  â†’ global_cond = (B, 2*(53+192)) = (B, 490)
```

---

## 4. 1D UNet: `DiffusionConditionalUnet1d`

### íƒ€ì„ìŠ¤í… ì„ë² ë”©
```
timestep scalar (B,)
  â†’ SinusoidalPosEmb(128)    â†’ (B, 128)
  â†’ Linear(128, 512) + Mish
  â†’ Linear(512, 128)          â†’ (B, 128)   â† diffusion_step_embed_dim
```

### FiLM ì»¨ë””ì…”ë‹ ë²¡í„°
```
global_feature = cat([timestep_embed, global_cond], dim=-1)
ì˜ˆ: (B, 128 + 490) = (B, 618)   â† UNet ëª¨ë“  ResBlockì— ì£¼ì…
```

### UNet êµ¬ì¡° (down_dims=(512, 1024, 2048) ê¸°ì¤€)

```
ì…ë ¥: (B, 16, 11)  â†’ rearrange â†’ (B, 11, 16)   â† (batch, channels, time)

[Encoder]
  ResBlock(11â†’512)  + ResBlock(512â†’512)  + Downsample(stride=2)  â†’ (B, 512,  8)
  ResBlock(512â†’1024)+ ResBlock(1024â†’1024)+ Downsample(stride=2)  â†’ (B, 1024, 4)
  ResBlock(1024â†’2048)+ResBlock(2048â†’2048)+ Identity(ë§ˆì§€ë§‰=no down)â†’ (B, 2048, 4)

[Bottleneck]
  ResBlock(2048â†’2048) Ã— 2                                          â†’ (B, 2048, 4)

[Decoder] (skip connectionìœ¼ë¡œ Encoder ì¶œë ¥ê³¼ cat)
  cat+ResBlock(2048*2â†’1024)+ResBlock(1024â†’1024)+Upsample           â†’ (B, 1024, 8)
  cat+ResBlock(1024*2â†’512) +ResBlock(512â†’512)  +Identity(ë§ˆì§€ë§‰)   â†’ (B, 512, 16)

[final_conv]
  Conv1dBlock(512â†’512) + Conv1d(512â†’11)                            â†’ (B, 11, 16)
  â†’ rearrange                                                       â†’ (B, 16, 11)

ì¶œë ¥: (B, 16, 11)   â† horizon Ã— action_dim
```

**ê° ResBlock ë‚´ë¶€ (FiLM ëª¨ë“ˆ):**
```
x: (B, C, T)
  Conv1d â†’ GroupNorm â†’ Mish
  FiLM: Linear(cond_dim, C*2) â†’ scale, bias
  out = scale * out + bias          â† use_film_scale_modulation=True
  Conv1d â†’ GroupNorm â†’ Mish
  + residual conv
```

---

## 5. í•™ìŠµ ì‹œ ì†ì‹¤ ê³„ì‚°

```python
# 1) ëœë¤ ë…¸ì´ì¦ˆ ìƒì„±
eps = randn_like(action)   # (B, 16, 11)

# 2) ëœë¤ timestep ìƒ˜í”Œ
t ~ Uniform(0, num_train_timesteps=100)

# 3) Forward diffusion: clean actionì— ë…¸ì´ì¦ˆ ì¶”ê°€
noisy_action = noise_scheduler.add_noise(action, eps, t)

# 4) UNetìœ¼ë¡œ ë…¸ì´ì¦ˆ ì˜ˆì¸¡
pred = unet(noisy_action, t, global_cond)

# 5) MSE loss (prediction_type="epsilon": ë…¸ì´ì¦ˆ ì˜ˆì¸¡)
loss = MSE(pred, eps)

# 6) íŒ¨ë”© ì˜ì—­ ë§ˆìŠ¤í‚¹ (do_mask_loss_for_padding=Falseê°€ ê¸°ë³¸)
loss = loss * ~action_is_pad
```

---

## 6. ì¶”ë¡  ì‹œ ì•¡ì…˜ ìƒì„±

```
noise = randn(B, 16, 11)   â† ìˆœìˆ˜ ê°€ìš°ì‹œì•ˆ

DDPM ì—­ë°©í–¥ (t=99 â†’ t=0, 100 ìŠ¤í…):
  for t in [99, 98, ..., 0]:
      pred_noise = unet(x_t, t, global_cond)
      x_{t-1}   = denoise_step(x_t, pred_noise, t)
      x_{t-1}   = clip(x_{t-1}, -1, 1)   â† clip_sample=True

ìµœì¢… action_chunk: (B, 16, 11)
ì‹¤ì œ ì‹¤í–‰ ì•¡ì…˜: action_chunk[:, 1:9, :]   â† start=n_obs-1=1, end=1+n_action_steps=9
```

> **DDIM**ìœ¼ë¡œ ë°”ê¾¸ë©´ ì¶”ë¡ ì„ 100â†’10~20 ìŠ¤í…ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŒ (`noise_scheduler_type="DDIM"`, `num_inference_steps=10`)

---

## 7. RoboCasa í•™ìŠµ ì „ ì§„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… í•„ìˆ˜ í™•ì¸

| í•­ëª© | ê¸°ë³¸ê°’ | RoboCasa ìƒí™© | ê¶Œì¥ |
|------|--------|--------------|------|
| `crop_shape` | `(84, 84)` | 128Ã—128ì—ì„œ 43% ì˜ë¦¼. ê³µê°„ ì •ë³´ ì†ì‹¤ ìœ„í—˜ | `(112, 112)` ë˜ëŠ” `null` |
| `pretrained_backbone_weights` | `None` (scratch) | ë°ì´í„° 3000epë¡œ scratchëŠ” ëŠë¦¼ | `"IMAGENET1K_V1"` ê³ ë ¤ (ë‹¨, GroupNormê³¼ ì¶©ëŒ â†’ use_group_norm=False í•„ìš”) |
| state key | â€” | 53D ì „ë¶€ or ì„ íƒ | ì¤‘ë³µ/ë…¸ì´ì¦ˆ ë§ì€ í‚¤ ì œê±° ê¶Œì¥ (velocityë¥˜) |
| `down_dims` | `(512, 1024, 2048)` | ê°€ì¥ í° ì„¤ì •. ë©”ëª¨ë¦¬ ì£¼ì˜ | ìš°ì„  ìœ ì§€, OOM ë‚˜ë©´ `(256, 512, 1024)` |
| `horizon % 2^len(down_dims)` | â€” | 16 % 8 = 0 âœ“ | â€” |
| `n_groups` | `8` | down_dims ëª¨ë‘ 8ë¡œ ë‚˜ëˆ ì§ âœ“ | â€” |
| `clip_sample_range` | `1.0` | actionì´ MIN_MAX ì •ê·œí™”ë˜ë©´ [-1,1]ì´ë¯€ë¡œ OK | â€” |

### âš ï¸ pretrained_backbone_weightsì™€ use_group_norm ì¶©ëŒ

```python
# ì´ ì¡°í•©ì€ ì—ëŸ¬ ë‚¨ (modeling_diffusion.py:468-471)
pretrained_backbone_weights = "IMAGENET1K_V1"
use_group_norm = True   # â† ValueError ë°œìƒ

# pretrained ì“°ë ¤ë©´:
pretrained_backbone_weights = "IMAGENET1K_V1"
use_group_norm = False   # BatchNorm ìœ ì§€
```

### ğŸ“Œ state key ì„ íƒ ê°€ì´ë“œ

```
ì „ì²´ 53D:
  robot0_base_pos        (3)   â† ë² ì´ìŠ¤ ìœ„ì¹˜ (ê³ ì • í™˜ê²½ì´ë©´ ë…¸ì´ì¦ˆ)
  robot0_base_quat       (4)   â† ë² ì´ìŠ¤ ìì„¸
  robot0_eef_pos         (3)   â† end-effector ìœ„ì¹˜ â˜…ì¤‘ìš”
  robot0_eef_quat        (4)   â† end-effector ìì„¸ â˜…ì¤‘ìš”
  robot0_joint_pos       (7)   â˜…ì¤‘ìš”
  robot0_joint_vel       (7)   (velocityëŠ” ë…¸ì´ì¦ˆ ë§ìŒ, ì œê±° ê³ ë ¤)
  robot0_gripper_qpos    (2)   â˜…ì¤‘ìš”
  robot0_gripper_qvel    (2)   (ì œê±° ê³ ë ¤)
  robot0_joint_pos_cos   (7)   joint_posì˜ cos ì¸ì½”ë”© (ì¤‘ë³µ ê°€ëŠ¥)
  robot0_joint_pos_sin   (7)   joint_posì˜ sin ì¸ì½”ë”© (ì¤‘ë³µ ê°€ëŠ¥)
  robot0_base_to_eef_pos (3)   â† ìƒëŒ€ ìœ„ì¹˜, ìœ ìš©í•  ìˆ˜ ìˆìŒ
  robot0_base_to_eef_quat(4)   â† ìƒëŒ€ ìì„¸

ìµœì†Œ ì¶”ì²œ (16D):
  joint_pos(7) + eef_pos(3) + eef_quat(4) + gripper_qpos(2)

ì¤‘ê°„ ì¶”ì²œ (32D):
  ìœ„ 16D + joint_vel(7) + base_to_eef_pos(3) + base_to_eef_quat(4) + gripper_qvel(2)
```

### ğŸ”¢ global_cond_dim ê³„ì‚° (ì„¤ì •ì— ë”°ë¼)

```
feature_dim = spatial_softmax_num_keypoints * 2 = 32 * 2 = 64
img_feat    = feature_dim * n_cameras = 64 * 3 = 192
per_step    = state_dim + img_feat

state_dim=16 â†’ per_step=208 â†’ global_cond = 208*2 = 416 â†’ FiLM cond = 416+128 = 544
state_dim=32 â†’ per_step=224 â†’ global_cond = 224*2 = 448 â†’ FiLM cond = 448+128 = 576
state_dim=53 â†’ per_step=245 â†’ global_cond = 245*2 = 490 â†’ FiLM cond = 490+128 = 618
```

---

## 8. íŒŒì¼ ìœ„ì¹˜ ì •ë¦¬

| íŒŒì¼ | ì—­í•  |
|------|------|
| `policies/diffusion/configuration_diffusion.py` | ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ |
| `policies/diffusion/modeling_diffusion.py` | ëª¨ë¸ êµ¬í˜„ (DiffusionPolicy, DiffusionModel, UNet ë“±) |
| `policies/diffusion/processor_diffusion.py` | ì •ê·œí™”/ì—­ì •ê·œí™” ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ |
| `scripts/lerobot_train_robocasa.py` | RoboCasa ì „ìš© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (state merge í¬í•¨) |
| `utils/custom_utils.py` | `merge_state_subkeys`, `make_concat_state_collate_fn` |
